{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InfectionMedicineProteomics/BINN/blob/refactor-explain/BINN_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eMrZui8xurM"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/InfectionMedicineProteomics/BINN/main/docs/img/logo.png\" height=\"150\" align=\"right\" style=\"height:240px\">\n",
        "\n",
        "# BINN v0.1.0 notebook\n",
        "\n",
        "This notebook allows you to create, train and explain **biologically informed neural networks (BINNs)** on your own datasets. Just upload your data and follow the instructions below!\n",
        "\n",
        "[Erik Hartman, Aaron M. Scott, Christofer Karlsson, Tirthankar Mohanty, Suvi T. Vaara, Adam Linder, Lars Malmström & Johan Malmström. Interpreting biologically informed neural networks for enhanced proteomic biomarker discovery and pathway analysis\n",
        "*Nature Communications*, 2023](https://www.nature.com/articles/s41467-023-41146-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt6CUsTdnOSV"
      },
      "source": [
        "## 0. Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvFl0z2HhW7u",
        "outputId": "4ad17de2-f17d-4e99-e031-376aec7af239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading BINN\n",
            "Cloning into 'BINN'...\n",
            "remote: Enumerating objects: 372, done.\u001b[K\n",
            "remote: Counting objects: 100% (210/210), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 372 (delta 98), reused 133 (delta 64), pack-reused 162 (from 2)\u001b[K\n",
            "Receiving objects: 100% (372/372), 18.66 MiB | 12.01 MiB/s, done.\n",
            "Resolving deltas: 100% (178/178), done.\n",
            "Installing BINN\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for binn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Install the BINN package from GitHub\n",
        "if not os.path.exists(\"/content/BINN\"):\n",
        "  print(\"Downloading BINN\")\n",
        "  !git clone --single-branch --branch refactor-explain https://github.com/InfectionMedicineProteomics/BINN.git\n",
        "else:\n",
        "  print(\"BINN is already downloaded\")\n",
        "print(\"Installing BINN\")\n",
        "!pip install -q -e  BINN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKojsfpYhoI0"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/InfectionMedicineProteomics/BINN/main/docs/img/data_explanation.png\" height=\"300\" align=\"right\" style=\"height:240px\">\n",
        "\n",
        "## 1. Upload data\n",
        "\n",
        "The BINN takes 4 files:\n",
        "1. Your datamatrix\n",
        "2. Your design matrix\n",
        "3. Optional: a file describing the pathways which will be used to build the network.\n",
        "4. Optional: a mapping between the input format and the pathway format.\n",
        "\n",
        "Instead of providing 3 and 4, you can use pre-loaded pathway files. We currently support input in the format of UniProt IDs or miRBase and the Reactome pathway database for the underlying structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1tqJWukahtE0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# @markdown ### Job name\n",
        "job_name = \"binn_job\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### Input source: `uniprot`, `mirbase` or `custom`.  If `custom` you need will be prompted to upload your own mapping file\n",
        "input_source = \"uniprot\"  # @param [\"uniprot\", \"mirbase\", \"custom\"]\n",
        "\n",
        "# @markdown ### Pathways File Options: `reactome` or `custom`. If `custom` you need will be prompted to upload your own pathway file\n",
        "pathways_source = \"reactome\"  # @param [\"reactome\", \"custom\"]\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Entity column in data matrix.  This should match in your data file.\n",
        "entity_column = \"Protein\"  # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### Sample and group columns in design matrix. This should match in the header in your design file.\n",
        "group_column = \"group\"  # @param {type:\"string\"}\n",
        "sample_column = \"sample\"  # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### Source and target columns in pathways file. This should match the header in your pathways file.\n",
        "source_column = \"source\"  # @param {type:\"string\"}\n",
        "target_column = \"target\"  # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### Input and translation columns in mapping file. This should match the header in your mapping file.\n",
        "input_column = \"input\"  # @param {type:\"string\"}\n",
        "translation_column = \"translation\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "def read_file(file):\n",
        "  filename = list(file.keys())[0]\n",
        "  if filename.endswith(\".tsv\"):\n",
        "    return pd.read_csv(filename, sep=\"\\t\")\n",
        "  elif filename.endswith(\".csv\"):\n",
        "    return pd.read_csv(filename)\n",
        "\n",
        "print(\"Please upload your input datamatrix:\")\n",
        "datamatrix = read_file(files.upload())\n",
        "\n",
        "print(\"Please upload your input designmatrix:\")\n",
        "designmatrix = read_file(files.upload())\n",
        "\n",
        "def validate_columns(df, required_cols, df_name):\n",
        "    \"\"\"\n",
        "    Check if required columns exist in the DataFrame.\n",
        "    Raise ValueError if any are missing.\n",
        "    \"\"\"\n",
        "    missing = [col for col in required_cols if col not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(\n",
        "            f\"Missing columns {missing} in {df_name}. \"\n",
        "            f\"Columns found: {list(df.columns)}\"\n",
        "        )\n",
        "\n",
        "\n",
        "if input_source == \"custom\":\n",
        "  print(\"Please upload your custom mapping file:\")\n",
        "  uploaded_mapping = files.upload()\n",
        "  mapping = read_file(uploaded_mapping)\n",
        "  print(f\"Loaded custom mapping file\")\n",
        "  validate_columns(mapping, [source_column, target_column], \"mapping\")\n",
        "\n",
        "\n",
        "if pathways_source == \"custom\":\n",
        "  print(\"Please upload your custom pathways file:\")\n",
        "  uploaded_pathways = files.upload()\n",
        "  pathways = read_file(uploaded_pathways)\n",
        "  print(f\"Loaded custom pathways file\")\n",
        "  validate_columns(pathways, [input_column, translation_column], \"pathways\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsRYQGDbiJ6s"
      },
      "source": [
        "## 2. Configure the BINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "tOv0CXkcosbT"
      },
      "outputs": [],
      "source": [
        "# @markdown ### BINN options\n",
        "\n",
        "n_layers = 4  # @param {type:\"integer\"}\n",
        "n_outputs = 2  # @param {type:\"integer\"}\n",
        "activation = \"tanh\" # @param\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Training options\n",
        "\n",
        "num_epochs = 50  # @param {type:\"integer\"}\n",
        "batch_size = 32  # @param {type:\"integer\"}\n",
        "validation_split = .2  # @param {type:\"slider\", min:0, max:1, step:0.01}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ50sQoj365y"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MWC4aFP9iM0a"
      },
      "outputs": [],
      "source": [
        "# @title # 3. Train the BINN!\n",
        "# Initialize the BINN model\n",
        "from binn import BINN, BINNDataLoader, BINNTrainer\n",
        "\n",
        "if input_source == \"uniprot\" and pathways_source == \"reactome\":\n",
        "  binn = BINN(data_matrix=datamatrix, network_source=\"reactome\", input_source=\"uniprot\", device=\"cpu\"\n",
        "  , n_outputs=n_outputs, n_layers=n_layers, activation=activation)\n",
        "else:\n",
        "  binn = BINN(data_matrix=datamatrix, mapping=mapping, pathways=pathways, device=\"cpu\",\n",
        "              n_outputs=n_outputs, n_layers=n_layers, activation=activation,\n",
        "              input_col=input_column, target_col=target_column, entity_col=entity_column,\n",
        "              source_col=source_column, translation_col=translation_column)\n",
        "\n",
        "\n",
        "binn_dataloader = BINNDataLoader(binn)\n",
        "\n",
        "dataloaders = binn_dataloader.create_dataloaders(\n",
        "    data_matrix=datamatrix,\n",
        "    design_matrix=designmatrix,\n",
        "    feature_column=entity_column,\n",
        "    group_column=group_column,\n",
        "    sample_column=sample_column,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=validation_split,\n",
        ")\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = BINNTrainer(binn)\n",
        "\n",
        "# Train the model\n",
        "trainer.fit(dataloaders=dataloaders, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-bncpvjiPXs"
      },
      "source": [
        "## 4. Explain the BINN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SIcNkNNrvrfs"
      },
      "outputs": [],
      "source": [
        "# @markdown ### Explainer options\n",
        "\n",
        "single_or_average = \"single\"  # @param [\"single\", \"average\"]\n",
        "normalization_method = \"fan\"  # @param [\"subgraph\", \"fan\"]\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### Options for average explainer\n",
        "n_iterations = 3  # @param {type:\"integer\"}\n",
        "num_epochs_explain = 50 # @param {type:\"integer\"}\n",
        "\n",
        "from binn import BINNExplainer\n",
        "\n",
        "explainer = BINNExplainer(binn)\n",
        "\n",
        "if single_or_average == \"average\":\n",
        "  explanations = explainer.explain(\n",
        "      dataloaders,\n",
        "      nr_iterations=n_iterations,\n",
        "      trainer=trainer,\n",
        "      num_epochs=num_epochs_explain,\n",
        "      normalization_method=normalization_method\n",
        "  )\n",
        "explanations.to_csv(\"/content/explanations.csv\")\n",
        "explanations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0oTNPB-zxqCi"
      },
      "outputs": [],
      "source": [
        "# @title ## 5. Plot\n",
        "from binn.plot.network import visualize_binn\n",
        "\n",
        "layer_specific_top_n = {\"0\": 10, \"1\": 7, \"2\": 5, \"3\": 5, \"4\": 5}\n",
        "\n",
        "plt = visualize_binn(\n",
        "    explanations,\n",
        "    top_n=layer_specific_top_n,\n",
        "    plot_size=(20, 10),\n",
        "    sink_node_size=500,\n",
        "    node_size_scaling=200,\n",
        "    edge_width=1,\n",
        "    node_cmap=\"coolwarm\"\n",
        ")\n",
        "\n",
        "plt.title(\"Interpreted network\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JmoWagdWvNKh"
      },
      "outputs": [],
      "source": [
        "# @title ## 6. Download\n",
        "files.download(f\"/content/{job_name}.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMTeMw3K7fAbIoqd93iiM2S",
      "collapsed_sections": [
        "Kt6CUsTdnOSV"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
