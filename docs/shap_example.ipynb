{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation and plotting\n",
    "\n",
    "The sparse networks created using BINN can be interpreted using various post-hoc interpretation methods. We chose to utilize SHAP to explain which nodes are important for classifications made by the BINN.\n",
    "\n",
    "Similar to in the [BINN example](../binn_example), we load some example data, and generate the network and the BINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binn import Network, BINN\n",
    "import pandas as pd\n",
    "\n",
    "input_data = pd.read_csv(\"../data/test_qm.csv\")\n",
    "translation = pd.read_csv(\"../data/translation.tsv\", sep=\"\\t\")\n",
    "pathways = pd.read_csv(\"../data/pathways.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Create the model\n",
    "network = Network(\n",
    "    input_data=input_data,\n",
    "    pathways=pathways,\n",
    "    mapping=translation,\n",
    ")\n",
    "\n",
    "binn = BINN(\n",
    "    pathways=network,\n",
    "    n_layers=4,\n",
    "    dropout=0.2,\n",
    "    validate=False,\n",
    "    residual=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load some test data and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Missing logger folder: /home/erikh/BINN/BINN/docs/lightning_logs\n",
      "\n",
      "  | Name   | Type             | Params\n",
      "--------------------------------------------\n",
      "0 | layers | Sequential       | 365 K \n",
      "1 | loss   | CrossEntropyLoss | 0     \n",
      "--------------------------------------------\n",
      "365 K     Trainable params\n",
      "0         Non-trainable params\n",
      "365 K     Total params\n",
      "1.464     Total estimated model params size (MB)\n",
      "The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 25/25 [00:00<00:00, 47.75it/s, loss=0.639, v_num=0, train_loss=0.640, train_acc=0.680]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 25/25 [00:00<00:00, 43.39it/s, loss=0.639, v_num=0, train_loss=0.640, train_acc=0.680]\n"
     ]
    }
   ],
   "source": [
    "from notebooks.util_for_examples import fit_data_matrix_to_network_input, generate_data\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "design_matrix = pd.read_csv('../data/design_matrix.tsv', sep=\"\\t\")\n",
    "protein_matrix = pd.read_csv('../data/test_qm.csv')\n",
    "\n",
    "protein_matrix = fit_data_matrix_to_network_input(\n",
    "    protein_matrix, features=network.inputs)\n",
    "\n",
    "X, y = generate_data(protein_matrix, design_matrix=design_matrix)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(torch.Tensor(X), torch.LongTensor(y)),\n",
    "                                            batch_size=8,\n",
    "                                            num_workers=12,\n",
    "                                            shuffle=True)\n",
    "trainer = Trainer(max_epochs=5)\n",
    "trainer.fit(binn, dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the BINNExplainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binn import BINNExplainer\n",
    "\n",
    "test_data = torch.Tensor(X)\n",
    "background_data = torch.Tensor(X)\n",
    "\n",
    "explainer = BINNExplainer(binn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the network using SHAP. The value column in the resulting dataframe contains the SHAP values. Note that each source and target entity is now superceded by the layer index. This is so that we can keep track of copies of nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>value</th>\n",
       "      <th>type</th>\n",
       "      <th>source layer</th>\n",
       "      <th>target layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0M8Q6_0</td>\n",
       "      <td>R-HSA-166663_1</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0M8Q6_0</td>\n",
       "      <td>R-HSA-166663_1</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0M8Q6_0</td>\n",
       "      <td>R-HSA-977606_1</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0M8Q6_0</td>\n",
       "      <td>R-HSA-977606_1</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0M8Q6_0</td>\n",
       "      <td>R-HSA-2029481_1</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source           target     value  type  source layer  target layer\n",
       "0  A0M8Q6_0   R-HSA-166663_1  0.000525     0             0             1\n",
       "1  A0M8Q6_0   R-HSA-166663_1  0.001632     1             0             1\n",
       "2  A0M8Q6_0   R-HSA-977606_1  0.000525     0             0             1\n",
       "3  A0M8Q6_0   R-HSA-977606_1  0.001632     1             0             1\n",
       "4  A0M8Q6_0  R-HSA-2029481_1  0.000525     0             0             1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df = explainer.explain(test_data, background_data)\n",
    "importance_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
