{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation and plotting\n",
    "\n",
    "The sparse networks created using BINN can be interpreted using various post-hoc interpretation methods. We chose to utilize SHAP to explain which nodes are important for classifications made by the BINN.\n",
    "\n",
    "Similar to in the [BINN example](../binn_example), we load some example data, and generate the network and the BINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erikh/BINN/BINN/test-venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from binn import Network, BINN\n",
    "import pandas as pd\n",
    "\n",
    "input_data = pd.read_csv(\"../data/test_qm.csv\")\n",
    "translation = pd.read_csv(\"../data/translation.tsv\", sep=\"\\t\")\n",
    "pathways = pd.read_csv(\"../data/pathways.tsv\", sep=\"\\t\")\n",
    "\n",
    "network = Network(\n",
    "    input_data=input_data,\n",
    "    pathways=pathways,\n",
    "    mapping=translation,\n",
    ")\n",
    "\n",
    "binn = BINN(\n",
    "    pathways=network,\n",
    "    n_layers=4,\n",
    "    dropout=0.2,\n",
    "    validate=False,\n",
    "    residual=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load some test data and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name   | Type             | Params\n",
      "--------------------------------------------\n",
      "0 | layers | Sequential       | 365 K \n",
      "1 | loss   | CrossEntropyLoss | 0     \n",
      "--------------------------------------------\n",
      "365 K     Trainable params\n",
      "0         Non-trainable params\n",
      "365 K     Total params\n",
      "1.464     Total estimated model params size (MB)\n",
      "The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 25/25 [00:00<00:00, 49.27it/s, loss=0.637, v_num=1, train_loss=0.635, train_acc=0.761]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 25/25 [00:00<00:00, 45.14it/s, loss=0.637, v_num=1, train_loss=0.635, train_acc=0.761]\n"
     ]
    }
   ],
   "source": [
    "from notebooks.util_for_examples import fit_data_matrix_to_network_input, generate_data\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "design_matrix = pd.read_csv('../data/design_matrix.tsv', sep=\"\\t\")\n",
    "protein_matrix = pd.read_csv('../data/test_qm.csv')\n",
    "\n",
    "protein_matrix = fit_data_matrix_to_network_input(\n",
    "    protein_matrix, features=network.inputs)\n",
    "\n",
    "X, y = generate_data(protein_matrix, design_matrix=design_matrix)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(torch.Tensor(X), torch.LongTensor(y)),\n",
    "                                            batch_size=8,\n",
    "                                            num_workers=12,\n",
    "                                            shuffle=True)\n",
    "trainer = Trainer(max_epochs=5)\n",
    "trainer.fit(binn, dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the BINNExplainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binn import BINNExplainer\n",
    "\n",
    "explainer = BINNExplainer(binn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the network using SHAP. The value column in the resulting dataframe contains the SHAP values. Note that each source and target entity is now superceded by the layer index. This is so that we can keep track of copies of nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>value</th>\n",
       "      <th>type</th>\n",
       "      <th>source layer</th>\n",
       "      <th>target layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0M8Q6_0</td>\n",
       "      <td>R-HSA-166663_1</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0M8Q6_0</td>\n",
       "      <td>R-HSA-166663_1</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0M8Q6_0</td>\n",
       "      <td>R-HSA-977606_1</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0M8Q6_0</td>\n",
       "      <td>R-HSA-977606_1</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0M8Q6_0</td>\n",
       "      <td>R-HSA-2029481_1</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source           target     value  type  source layer  target layer\n",
       "0  A0M8Q6_0   R-HSA-166663_1  0.001106     0             0             1\n",
       "1  A0M8Q6_0   R-HSA-166663_1  0.001248     1             0             1\n",
       "2  A0M8Q6_0   R-HSA-977606_1  0.001106     0             0             1\n",
       "3  A0M8Q6_0   R-HSA-977606_1  0.001248     1             0             1\n",
       "4  A0M8Q6_0  R-HSA-2029481_1  0.001106     0             0             1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = torch.Tensor(X)\n",
    "background_data = torch.Tensor(X)\n",
    "\n",
    "importance_df = explainer.explain(test_data, background_data)\n",
    "importance_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance df can be used to create an importance network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binn import ImportanceNetwork\n",
    "\n",
    "IG = ImportanceNetwork(importance_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance network can then be used to generate plots. Here we generate an upstream Sankey plot originating from the node 'P02766'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "query_node = 'P02766'\n",
    "\n",
    "IG.generate_sankey(query_node, upstream=False, savename='img/sankey.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/sankey.png\" width=\"250\" height=\"250\" style=\"display:block;margin-left:auto;margin-right:auto;\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the explainer for several iterations and compute the average importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erikh/BINN/BINN/test-venv/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:107: PossibleUserWarning:\n",
      "\n",
      "You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "/home/erikh/BINN/BINN/test-venv/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning:\n",
      "\n",
      "Checkpoint directory /home/erikh/BINN/BINN/docs/lightning_logs/version_1/checkpoints exists and is not empty.\n",
      "\n",
      "\n",
      "  | Name   | Type             | Params\n",
      "--------------------------------------------\n",
      "0 | layers | Sequential       | 365 K \n",
      "1 | loss   | CrossEntropyLoss | 0     \n",
      "--------------------------------------------\n",
      "365 K     Trainable params\n",
      "0         Non-trainable params\n",
      "365 K     Total params\n",
      "1.464     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "/home/erikh/BINN/BINN/test-venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning:\n",
      "\n",
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/erikh/BINN/BINN/docs/shap_example.ipynb Cell 15\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/erikh/BINN/BINN/docs/shap_example.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m average_importance_df \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mexplain_average(test_data \u001b[39m=\u001b[39;49m test_data, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/erikh/BINN/BINN/docs/shap_example.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                                                   background_data \u001b[39m=\u001b[39;49m background_data, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/erikh/BINN/BINN/docs/shap_example.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                                                   nr_iterations\u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/erikh/BINN/BINN/docs/shap_example.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                                                   trainer\u001b[39m=\u001b[39;49mtrainer,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/erikh/BINN/BINN/docs/shap_example.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                                                   dataloader\u001b[39m=\u001b[39;49m dataloader)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/erikh/BINN/BINN/docs/shap_example.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m average_importance_df\n",
      "File \u001b[0;32m~/BINN/BINN/binn/explainer.py:100\u001b[0m, in \u001b[0;36mBINNExplainer.explain_average\u001b[0;34m(self, test_data, background_data, nr_iterations, trainer, dataloader)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39minit_weights()\n\u001b[1;32m     99\u001b[0m     trainer\u001b[39m.\u001b[39mfit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, dataloader)\n\u001b[0;32m--> 100\u001b[0m     df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplain(test_data, background_data)\n\u001b[1;32m    101\u001b[0m     dfs[iteration] \u001b[39m=\u001b[39m df\n\u001b[1;32m    103\u001b[0m col_names \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalue_\u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(dfs\u001b[39m.\u001b[39mkeys())))]\n",
      "File \u001b[0;32m~/BINN/BINN/binn/explainer.py:32\u001b[0m, in \u001b[0;36mBINNExplainer.explain\u001b[0;34m(self, test_data, background_data)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexplain\u001b[39m(\u001b[39mself\u001b[39m, test_data: torch\u001b[39m.\u001b[39mTensor, background_data: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     21\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m    Generates SHAP explanations for a given test_data by computing the Shapley values for each feature using\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m    the provided background_data. The feature importances are then aggregated and returned in a pandas dataframe.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m        pd.DataFrame: A dataframe containing the aggregated SHAP feature importances.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     shap_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_explain_layers(background_data, test_data)\n\u001b[1;32m     34\u001b[0m     feature_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m     35\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m     36\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtarget layer\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m     41\u001b[0m     }\n\u001b[1;32m     42\u001b[0m     connectivity_matrices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_connectivity_matrices()\n",
      "File \u001b[0;32m~/BINN/BINN/binn/explainer.py:167\u001b[0m, in \u001b[0;36mBINNExplainer._explain_layers\u001b[0;34m(self, background_data, test_data)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    163\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mResidual\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m name \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mfinal\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m name\n\u001b[1;32m    164\u001b[0m ):\n\u001b[1;32m    165\u001b[0m     explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mDeepExplainer(\n\u001b[1;32m    166\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, layer), background_data)\n\u001b[0;32m--> 167\u001b[0m     shap_values \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mshap_values(test_data)\n\u001b[1;32m    168\u001b[0m     shap_dict[\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    169\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mlayer_names[feature_index])\n\u001b[1;32m    170\u001b[0m     shap_dict[\u001b[39m\"\u001b[39m\u001b[39mshap_values\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(shap_values)\n",
      "File \u001b[0;32m~/BINN/BINN/test-venv/lib/python3.9/site-packages/shap/explainers/_deep/__init__.py:124\u001b[0m, in \u001b[0;36mDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshap_values\u001b[39m(\u001b[39mself\u001b[39m, X, ranked_outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, output_rank_order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, check_additivity\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     91\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m        were chosen as \"top\".\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplainer\u001b[39m.\u001b[39;49mshap_values(X, ranked_outputs, output_rank_order, check_additivity\u001b[39m=\u001b[39;49mcheck_additivity)\n",
      "File \u001b[0;32m~/BINN/BINN/test-venv/lib/python3.9/site-packages/shap/explainers/_deep/deep_pytorch.py:185\u001b[0m, in \u001b[0;36mPyTorchDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39m# run attribution computation graph\u001b[39;00m\n\u001b[1;32m    184\u001b[0m feature_ind \u001b[39m=\u001b[39m model_output_ranks[j, i]\n\u001b[0;32m--> 185\u001b[0m sample_phis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient(feature_ind, joint_x)\n\u001b[1;32m    186\u001b[0m \u001b[39m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterim:\n",
      "File \u001b[0;32m~/BINN/BINN/test-venv/lib/python3.9/site-packages/shap/explainers/_deep/deep_pytorch.py:103\u001b[0m, in \u001b[0;36mPyTorchDeep.gradient\u001b[0;34m(self, idx, inputs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    102\u001b[0m X \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mrequires_grad_() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m inputs]\n\u001b[0;32m--> 103\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49mX)\n\u001b[1;32m    104\u001b[0m selected \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m outputs[:, idx]]\n\u001b[1;32m    105\u001b[0m grads \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/BINN/BINN/test-venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/BINN/BINN/binn/binn.py:117\u001b[0m, in \u001b[0;36mBINN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mPerforms a forward pass through the BINN.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m    torch.Tensor: The output tensor of the BINN.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual:\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m _forward_residual(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers, x)\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers(x)\n",
      "File \u001b[0;32m~/BINN/BINN/binn/binn.py:386\u001b[0m, in \u001b[0;36m_forward_residual\u001b[0;34m(model, x)\u001b[0m\n\u001b[1;32m    384\u001b[0m             residual_counter \u001b[39m=\u001b[39m residual_counter \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    385\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m    387\u001b[0m x_final \u001b[39m=\u001b[39m x_final \u001b[39m/\u001b[39m (residual_counter)\n\u001b[1;32m    389\u001b[0m \u001b[39mreturn\u001b[39;00m x_final\n",
      "File \u001b[0;32m~/BINN/BINN/test-venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1146\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1148\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1150\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/BINN/BINN/test-venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "average_importance_df = explainer.explain_average(test_data = test_data, \n",
    "                                                  background_data = background_data, \n",
    "                                                  nr_iterations= 10, \n",
    "                                                  trainer=trainer,\n",
    "                                                  dataloader= dataloader)\n",
    "average_importance_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
